---
title: "Python Mastery Part 7: Performance Optimization & Production Best Practices"
date: 2024-12-08 00:00:00 +0530
categories: [Python, Python Mastery]
tags: [Python, Production, Best Practices, Performance, Optimization, Profiling, Caching, Security]
---
## Table of Contents

- [Introduction](#introduction)
- [7.1 Performance Profiling](#71-performance-profiling)
  - [CPU Profiling](#cpu-profiling)
  - [Memory Profiling](#memory-profiling)
  - [Line-by-Line Profiling](#line-by-line-profiling)
  - [Production Profiling](#production-profiling)
  - [Frequently Asked Questions](#frequently-asked-questions)
  - [Interview Questions](#interview-questions)
- [7.2 Performance Optimization](#72-performance-optimization)
  - [Algorithm Optimization](#algorithm-optimization)
  - [Data Structure Selection](#data-structure-selection)
  - [Caching Strategies](#caching-strategies)
  - [Database Optimization](#database-optimization)
  - [Frequently Asked Questions](#frequently-asked-questions-1)
  - [Interview Questions](#interview-questions-1)
- [7.3 Concurrency Optimization](#73-concurrency-optimization)
- [7.4 Production Security](#74-production-security)
- [7.5 Production Readiness](#75-production-readiness)

---

# Complete Python Mastery Part 7: Performance Optimization & Production Best Practices

## Introduction

Welcome to **Part 7** of the Complete Python Mastery series. Performance optimization and production best practices are critical for running scalable, secure applications.

**What You'll Learn in Part 7:**

This post covers performance optimization and production readiness:

- **Performance profiling**: CPU, memory, and production profiling
- **Optimization techniques**: Algorithms, data structures, caching
- **Concurrency optimization**: Threading, multiprocessing, async
- **Production security**: Authentication, encryption, vulnerability scanning
- **Production readiness**: Checklists, monitoring, incident response

**Why This Matters:**

Production-ready applications require:

- Optimal performance under load
- Efficient resource utilization
- Security against attacks
- Reliability and uptime
- Observability for debugging

**Prerequisites:**

- Parts 1-6 of this series
- Understanding of Python fundamentals
- Basic system administration knowledge

Let's optimize for production! üöÄ

---

## 7.1 Performance Profiling

**SDLC Phase:** Development, Testing, Maintenance

**Relevant For:**

- [ ] Requirements gathering
- [ ] System design
- [X] Development (optimization)
- [X] Testing (performance testing)
- [ ] Deployment
- [X] Maintenance (troubleshooting)

### CPU Profiling

```python
"""
CPU PROFILING: Identify performance bottlenecks

Tools:
- cProfile: Built-in profiler
- profile: Pure Python profiler (slower)
- py-spy: Sampling profiler (production-safe)
"""

# Example: Profile slow function
import cProfile
import pstats
from io import StringIO

def slow_function():
    """Intentionally slow function"""
    total = 0
    for i in range(1000000):
        total += i * i
    return total

def process_data(data):
    """Process data with multiple steps"""
    # Step 1: Parse
    parsed = [parse_item(item) for item in data]
  
    # Step 2: Validate
    validated = [validate_item(item) for item in parsed]
  
    # Step 3: Transform
    transformed = [transform_item(item) for item in validated]
  
    return transformed

# Profile with cProfile
def profile_function():
    profiler = cProfile.Profile()
    profiler.enable()
  
    # Run code to profile
    result = slow_function()
  
    profiler.disable()
  
    # Print stats
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # Top 10 functions

# Output:
"""
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.123    0.123    0.123    0.123 script.py:5(slow_function)
  1000000    0.089    0.000    0.089    0.000 {built-in method builtins.range}
        1    0.000    0.000    0.123    0.123 <string>:1(<module>)
"""

# Interpretation:
"""
- tottime: Total time in function (excluding subcalls)
- cumtime: Total time including subcalls
- ncalls: Number of calls
- percall: Average time per call

Focus on functions with:
‚úÖ High tottime (self time)
‚úÖ High cumtime (total time)
‚úÖ Many calls (optimization opportunity)
"""

# Profile decorator
import functools
import time

def profile(func):
    """Decorator to profile function"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
      
        result = func(*args, **kwargs)
      
        profiler.disable()
      
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats()
      
        return result
    return wrapper

@profile
def my_function():
    """Function to profile"""
    return slow_function()

# Context manager for profiling
from contextlib import contextmanager

@contextmanager
def profiled():
    """Context manager for profiling code block"""
    profiler = cProfile.Profile()
    profiler.enable()
    yield
    profiler.disable()
  
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)

# Usage
with profiled():
    result = slow_function()

# Command-line profiling
# $ python -m cProfile -s cumulative script.py

# Save profile for later analysis
# $ python -m cProfile -o output.prof script.py

# Visualize with snakeviz
# $ pip install snakeviz
# $ snakeviz output.prof
```

### Memory Profiling

```python
"""
MEMORY PROFILING: Find memory leaks and excessive usage

Tools:
- memory_profiler: Line-by-line memory usage
- tracemalloc: Built-in memory tracer
- guppy3: Heap analysis
"""

# Install: pip install memory-profiler

from memory_profiler import profile

@profile
def memory_intensive():
    """Memory-intensive function"""
    # Create large list
    data = [i for i in range(1000000)]
  
    # Create dictionary
    mapping = {i: i**2 for i in range(100000)}
  
    # Process data
    result = [x * 2 for x in data]
  
    return result

# Run: python -m memory_profiler script.py

# Output:
"""
Line #    Mem usage    Increment   Line Contents
================================================
     3   38.5 MiB     38.5 MiB   @profile
     4                             def memory_intensive():
     5   46.2 MiB      7.7 MiB       data = [i for i in range(1000000)]
     6   54.8 MiB      8.6 MiB       mapping = {i: i**2 for i in range(100000)}
     7   62.5 MiB      7.7 MiB       result = [x * 2 for x in data]
     8   62.5 MiB      0.0 MiB       return result
"""

# Built-in tracemalloc
import tracemalloc

def track_memory():
    """Track memory allocations"""
    # Start tracking
    tracemalloc.start()
  
    # Code to track
    data = [i for i in range(1000000)]
  
    # Get current memory
    current, peak = tracemalloc.get_traced_memory()
    print(f"Current: {current / 1024 / 1024:.2f} MB")
    print(f"Peak: {peak / 1024 / 1024:.2f} MB")
  
    # Get top memory consumers
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
  
    print("\nTop 10 memory consumers:")
    for stat in top_stats[:10]:
        print(stat)
  
    tracemalloc.stop()

# Find memory leaks
def find_memory_leaks():
    """Compare memory snapshots to find leaks"""
    tracemalloc.start()
  
    # Take snapshot before
    snapshot1 = tracemalloc.take_snapshot()
  
    # Code that might leak
    leaked_data = []
    for i in range(1000):
        leaked_data.append([0] * 10000)  # Never released!
  
    # Take snapshot after
    snapshot2 = tracemalloc.take_snapshot()
  
    # Compare snapshots
    top_stats = snapshot2.compare_to(snapshot1, 'lineno')
  
    print("Top memory increases:")
    for stat in top_stats[:10]:
        print(stat)
  
    tracemalloc.stop()

# Monitor object creation
import sys

def get_object_sizes():
    """Get size of Python objects"""
    data = [1, 2, 3, 4, 5]
    mapping = {'a': 1, 'b': 2}
    text = "Hello, World!"
  
    print(f"List: {sys.getsizeof(data)} bytes")
    print(f"Dict: {sys.getsizeof(mapping)} bytes")
    print(f"String: {sys.getsizeof(text)} bytes")
  
    # Deep size (including contents)
    def get_deep_size(obj, seen=None):
        """Recursive size calculation"""
        size = sys.getsizeof(obj)
        if seen is None:
            seen = set()
      
        obj_id = id(obj)
        if obj_id in seen:
            return 0
      
        seen.add(obj_id)
      
        if isinstance(obj, dict):
            size += sum(get_deep_size(v, seen) for v in obj.values())
            size += sum(get_deep_size(k, seen) for k in obj.keys())
        elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
            size += sum(get_deep_size(i, seen) for i in obj)
      
        return size
  
    nested = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
    print(f"Nested list (shallow): {sys.getsizeof(nested)} bytes")
    print(f"Nested list (deep): {get_deep_size(nested)} bytes")
```

### Line-by-Line Profiling

```python
"""
LINE-BY-LINE PROFILING: Identify exact slow lines

Tool: line_profiler
Install: pip install line-profiler
"""

from line_profiler import LineProfiler

def slow_loop():
    """Function with slow loop"""
    total = 0
  
    # Slow: String concatenation in loop
    result = ""
    for i in range(10000):
        result += str(i)  # Very slow!
  
    # Slow: Repeated list append
    numbers = []
    for i in range(100000):
        numbers.append(i)
  
    return result, numbers

def profile_lines():
    """Profile line by line"""
    lp = LineProfiler()
    lp.add_function(slow_loop)
    lp.run('slow_loop()')
    lp.print_stats()

# Output:
"""
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     5                                           def slow_loop():
     6         1          2.0      2.0      0.0      total = 0
     7         1          1.0      1.0      0.0      result = ""
     8     10001      15847.0      1.6      2.1      for i in range(10000):
     9     10000     745231.0     74.5     97.9          result += str(i)  # Very slow!
"""

# Command-line usage
# $ kernprof -l -v script.py

# Optimize slow lines
def optimized_loop():
    """Optimized version"""
    total = 0
  
    # Fast: Use join
    result = "".join(str(i) for i in range(10000))
  
    # Fast: List comprehension
    numbers = [i for i in range(100000)]
  
    return result, numbers

# Benchmark comparison
import timeit

slow_time = timeit.timeit(slow_loop, number=10)
fast_time = timeit.timeit(optimized_loop, number=10)

print(f"Slow: {slow_time:.4f}s")
print(f"Fast: {fast_time:.4f}s")
print(f"Speedup: {slow_time / fast_time:.2f}x")
```

### Production Profiling

```python
"""
PRODUCTION PROFILING: Profile live applications safely

Tools:
- py-spy: Sampling profiler (low overhead)
- pyinstrument: Call stack profiler
- Application Performance Monitoring (APM)
"""

# py-spy (doesn't require code changes!)
# Install: pip install py-spy

# Sample running process
# $ py-spy top --pid 12345

# Generate flame graph
# $ py-spy record -o profile.svg --pid 12345

# Record for 60 seconds
# $ py-spy record -o profile.svg --duration 60 --pid 12345

# pyinstrument (low overhead)
from pyinstrument import Profiler

profiler = Profiler()
profiler.start()

# Code to profile
result = slow_function()

profiler.stop()
print(profiler.output_text(unicode=True, color=True))

# FastAPI middleware for profiling
from fastapi import FastAPI, Request
from pyinstrument import Profiler
import time

app = FastAPI()

@app.middleware("http")
async def profile_request(request: Request, call_next):
    """Profile slow requests"""
    profiling = request.query_params.get("profile", False)
  
    if profiling:
        profiler = Profiler()
        profiler.start()
      
        response = await call_next(request)
      
        profiler.stop()
      
        # Add profile to response header
        response.headers["X-Profile"] = profiler.output_text()
      
        return response
  
    return await call_next(request)

# Access with: http://localhost:8000/api/users?profile=true

# Continuous profiling in production
import os
import sys

if os.getenv("ENABLE_PROFILING") == "true":
    from pyinstrument import Profiler
  
    profiler = Profiler()
    profiler.start()
  
    # Register exit handler
    import atexit
  
    def save_profile():
        profiler.stop()
        with open('profile.html', 'w') as f:
            f.write(profiler.output_html())
  
    atexit.register(save_profile)
```

### Frequently Asked Questions

**Q1: When should I optimize code?**

**A:**

```python
"""
OPTIMIZATION GUIDELINES

Golden rules:
1. "Premature optimization is the root of all evil" - Donald Knuth
2. Make it work, make it right, make it fast (in that order)
3. Profile before optimizing

When to optimize:
‚úÖ Performance is a requirement (SLA, user experience)
‚úÖ Profiling shows bottleneck
‚úÖ Code is working correctly
‚úÖ Tests are passing

When NOT to optimize:
‚ùå Before code works
‚ùå Without profiling
‚ùå When performance is acceptable
‚ùå When it hurts readability significantly

Optimization workflow:
1. Make it work (functionality)
2. Make it right (correctness, tests)
3. Profile (find bottlenecks)
4. Optimize (targeted improvements)
5. Verify (benchmark, profile again)
"""

# Example: Premature optimization (BAD)
# ‚ùå BAD: Optimizing before it works
def calculate_fibonacci(n):
    # Spent 2 hours optimizing with memoization
    # But algorithm is wrong!
    memo = {}
    def fib(x):
        if x in memo:
            return memo[x]
        if x <= 1:
            return x
        result = fib(x-1) + fib(x-2)  # Bug: off by one
        memo[x] = result
        return result
    return fib(n)

# ‚úÖ GOOD: Optimize after profiling
# Step 1: Make it work
def calculate_fibonacci_v1(n):
    """Simple, correct implementation"""
    if n <= 1:
        return n
    return calculate_fibonacci_v1(n-1) + calculate_fibonacci_v1(n-2)

# Step 2: Verify correctness
assert calculate_fibonacci_v1(10) == 55

# Step 3: Profile (found it's slow for n>30)

# Step 4: Optimize
from functools import lru_cache

@lru_cache(maxsize=None)
def calculate_fibonacci_v2(n):
    """Optimized with caching"""
    if n <= 1:
        return n
    return calculate_fibonacci_v2(n-1) + calculate_fibonacci_v2(n-2)

# Step 5: Verify improvement
import timeit
slow = timeit.timeit(lambda: calculate_fibonacci_v1(30), number=1)
fast = timeit.timeit(lambda: calculate_fibonacci_v2(30), number=1)
print(f"Speedup: {slow / fast:.2f}x")  # 1000x faster!

# Decision framework:
"""
Is performance acceptable? ‚Üí No optimization needed
    ‚Üì No
Profile to find bottleneck
    ‚Üì
Can you use better algorithm? ‚Üí Try that first
    ‚Üì No
Can you cache results? ‚Üí Try caching
    ‚Üì No
Can you use better data structure? ‚Üí Try that
    ‚Üì No
Can you parallelize? ‚Üí Try async/threading/multiprocessing
    ‚Üì No
Can you optimize hot loops? ‚Üí Try Cython/PyPy
    ‚Üì No
Can you scale horizontally? ‚Üí Add more servers
"""
```

**Why This Matters:** Premature optimization wastes time and hurts code quality.

---

### Interview Questions

**Question 1: How would you identify and fix a memory leak in a Python application?**

**Difficulty:** Senior-Level

**SDLC Relevance:** Maintenance, Production Operations

**Answer:**

```python
"""
IDENTIFYING AND FIXING MEMORY LEAKS

Step-by-step approach:
1. Confirm memory leak exists
2. Use profiling tools
3. Analyze object growth
4. Identify leak source
5. Fix and verify
"""

# Step 1: Confirm leak with monitoring
import psutil
import os
import time

def monitor_memory(duration=60):
    """Monitor memory usage over time"""
    process = psutil.Process(os.getpid())
  
    memory_samples = []
    for i in range(duration):
        mem = process.memory_info().rss / 1024 / 1024  # MB
        memory_samples.append(mem)
        print(f"Memory: {mem:.2f} MB")
        time.sleep(1)
  
    # Check if memory is growing
    growth = memory_samples[-1] - memory_samples[0]
    if growth > 100:  # More than 100 MB growth
        print("‚ö†Ô∏è Potential memory leak detected!")

# Step 2: Use tracemalloc to find leak
import tracemalloc

def find_leak():
    """Track memory allocation"""
    tracemalloc.start()
  
    # Take initial snapshot
    snapshot1 = tracemalloc.take_snapshot()
  
    # Run code that might leak
    leaked_objects = []
    for i in range(1000):
        # BUG: Objects never released!
        leaked_objects.append(LargeObject())
  
    # Take second snapshot
    snapshot2 = tracemalloc.take_snapshot()
  
    # Find differences
    top_stats = snapshot2.compare_to(snapshot1, 'lineno')
  
    print("Top 10 memory increases:")
    for stat in top_stats[:10]:
        print(stat)
  
    tracemalloc.stop()

# Common leak patterns:

# 1. Global containers that grow unbounded
# ‚ùå BAD: Unbounded cache
cache = {}  # Global

def get_data(key):
    if key not in cache:
        cache[key] = expensive_computation(key)  # Grows forever!
    return cache[key]

# ‚úÖ GOOD: Bounded cache
from functools import lru_cache

@lru_cache(maxsize=1000)  # Limited size
def get_data(key):
    return expensive_computation(key)

# 2. Circular references
# ‚ùå BAD: Circular reference
class Node:
    def __init__(self):
        self.parent = None
        self.children = []
  
    def add_child(self, child):
        child.parent = self  # Circular reference!
        self.children.append(child)

# ‚úÖ GOOD: Use weak references
import weakref

class Node:
    def __init__(self):
        self.parent = None  # Will use weak reference
        self.children = []
  
    def add_child(self, child):
        child.parent = weakref.ref(self)  # Weak reference
        self.children.append(child)

# 3. Unclosed resources
# ‚ùå BAD: File not closed
def process_file(filename):
    f = open(filename)
    data = f.read()  # File never closed!
    return data

# ‚úÖ GOOD: Use context manager
def process_file(filename):
    with open(filename) as f:
        data = f.read()
    return data  # File automatically closed

# 4. Event listeners not removed
# ‚ùå BAD: Listeners accumulate
class EventEmitter:
    def __init__(self):
        self.listeners = []
  
    def on(self, callback):
        self.listeners.append(callback)  # Never removed!

# ‚úÖ GOOD: Remove listeners
class EventEmitter:
    def __init__(self):
        self.listeners = []
  
    def on(self, callback):
        self.listeners.append(callback)
  
    def off(self, callback):
        self.listeners.remove(callback)

# Step 5: Verify fix with test
def test_no_leak():
    """Test that fix prevents leak"""
    import gc
  
    # Force garbage collection
    gc.collect()
  
    # Get initial object count
    initial_count = len(gc.get_objects())
  
    # Run code 1000 times
    for _ in range(1000):
        result = get_data(random_key())
  
    # Force garbage collection
    gc.collect()
  
    # Get final object count
    final_count = len(gc.get_objects())
  
    # Check object growth is bounded
    growth = final_count - initial_count
    assert growth < 100, f"Memory leak: {growth} objects leaked"

# Production monitoring
"""
1. Set up memory alerts (e.g., > 80% memory usage)
2. Track memory metrics over time
3. Monitor object counts per type
4. Set up automated leak detection
5. Have rollback plan ready
"""
```

**Key Points:**

- Use tracemalloc to track allocations
- Common leaks: unbounded caches, circular references, unclosed resources
- Use weak references for circular structures
- Always use context managers for resources
- Verify fixes with tests
- Monitor memory in production

---

## 7.2 Performance Optimization

**SDLC Phase:** Development, Optimization

**Relevant For:**

- [ ] Requirements gathering
- [X] System design (performance requirements)
- [X] Development (optimization)
- [X] Testing (performance testing)
- [ ] Deployment
- [X] Maintenance (optimization)

### Algorithm Optimization

```python
"""
ALGORITHM OPTIMIZATION: Choose right algorithm

Time Complexity (Big O):
- O(1): Constant
- O(log n): Logarithmic
- O(n): Linear
- O(n log n): Linearithmic
- O(n¬≤): Quadratic
- O(2‚Åø): Exponential
"""

# Example: Find duplicates in list

# ‚ùå BAD: O(n¬≤) - nested loops
def find_duplicates_slow(items):
    """Quadratic time complexity"""
    duplicates = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            if items[i] == items[j] and items[i] not in duplicates:
                duplicates.append(items[i])
    return duplicates

# ‚úÖ GOOD: O(n) - use set
def find_duplicates_fast(items):
    """Linear time complexity"""
    seen = set()
    duplicates = set()
  
    for item in items:
        if item in seen:
            duplicates.add(item)
        seen.add(item)
  
    return list(duplicates)

# Benchmark
import timeit

data = list(range(1000)) + list(range(500))  # 1500 items with duplicates

slow_time = timeit.timeit(lambda: find_duplicates_slow(data), number=10)
fast_time = timeit.timeit(lambda: find_duplicates_fast(data), number=10)

print(f"Slow (O(n¬≤)): {slow_time:.4f}s")
print(f"Fast (O(n)): {fast_time:.4f}s")
print(f"Speedup: {slow_time / fast_time:.0f}x")  # 100x+ faster!

# Example 2: Search in sorted list

# ‚ùå BAD: O(n) - linear search
def linear_search(items, target):
    """Linear time"""
    for i, item in enumerate(items):
        if item == target:
            return i
    return -1

# ‚úÖ GOOD: O(log n) - binary search
def binary_search(items, target):
    """Logarithmic time"""
    left, right = 0, len(items) - 1
  
    while left <= right:
        mid = (left + right) // 2
      
        if items[mid] == target:
            return mid
        elif items[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
  
    return -1

# Benchmark
sorted_data = list(range(1000000))

linear_time = timeit.timeit(lambda: linear_search(sorted_data, 999999), number=1000)
binary_time = timeit.timeit(lambda: binary_search(sorted_data, 999999), number=1000)

print(f"Linear: {linear_time:.4f}s")
print(f"Binary: {binary_time:.4f}s")
print(f"Speedup: {linear_time / binary_time:.0f}x")  # 10,000x+ faster!

# Example 3: Fibonacci

# ‚ùå BAD: O(2‚Åø) - exponential
def fib_slow(n):
    """Exponential time - recalculates same values"""
    if n <= 1:
        return n
    return fib_slow(n-1) + fib_slow(n-2)

# ‚úÖ GOOD: O(n) - dynamic programming
def fib_fast(n, memo=None):
    """Linear time - memoization"""
    if memo is None:
        memo = {}
  
    if n in memo:
        return memo[n]
  
    if n <= 1:
        return n
  
    memo[n] = fib_fast(n-1, memo) + fib_fast(n-2, memo)
    return memo[n]

# ‚úÖ BETTER: O(n) - iterative
def fib_iterative(n):
    """Linear time - no recursion"""
    if n <= 1:
        return n
  
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b

# Benchmark
print(f"fib_slow(30): {timeit.timeit(lambda: fib_slow(30), number=1):.4f}s")
print(f"fib_fast(30): {timeit.timeit(lambda: fib_fast(30), number=1):.4f}s")
print(f"fib_iterative(30): {timeit.timeit(lambda: fib_iterative(30), number=1):.4f}s")
```

### Data Structure Selection

```python
"""
DATA STRUCTURE SELECTION: Choose right structure for operation

Common operations and best structures:
- Lookup by key: dict (O(1))
- Check membership: set (O(1))
- Maintain order: list (O(1) append)
- Sorted data: bisect/heapq
- FIFO queue: collections.deque
- Priority queue: heapq
"""

# Example: Count word frequency

# ‚ùå BAD: List of tuples (O(n) lookup)
def count_words_slow(words):
    """Slow: O(n) lookup for each word"""
    counts = []  # List of (word, count) tuples
  
    for word in words:
        found = False
        for i, (w, c) in enumerate(counts):
            if w == word:
                counts[i] = (w, c + 1)
                found = True
                break
      
        if not found:
            counts.append((word, 1))
  
    return counts

# ‚úÖ GOOD: Dictionary (O(1) lookup)
def count_words_fast(words):
    """Fast: O(1) lookup"""
    counts = {}
  
    for word in words:
        counts[word] = counts.get(word, 0) + 1
  
    return counts

# ‚úÖ BETTER: Use Counter
from collections import Counter

def count_words_better(words):
    """Best: Built-in optimized"""
    return Counter(words)

# Example: Remove duplicates while maintaining order

# ‚ùå BAD: List with repeated lookups
def unique_slow(items):
    """O(n¬≤) - list lookup is O(n)"""
    result = []
    for item in items:
        if item not in result:  # O(n) lookup!
            result.append(item)
    return result

# ‚úÖ GOOD: Set for tracking, list for order
def unique_fast(items):
    """O(n) - set lookup is O(1)"""
    seen = set()
    result = []
  
    for item in items:
        if item not in seen:  # O(1) lookup!
            seen.add(item)
            result.append(item)
  
    return result

# ‚úÖ BETTER: dict (maintains insertion order in Python 3.7+)
def unique_better(items):
    """O(n) - dict is ordered since Python 3.7"""
    return list(dict.fromkeys(items))

# Example: Task queue

# ‚ùå BAD: List as queue (O(n) for pop(0))
queue = []
queue.append(task1)  # O(1)
queue.append(task2)  # O(1)
task = queue.pop(0)  # O(n) - shifts all elements!

# ‚úÖ GOOD: deque (O(1) for both ends)
from collections import deque

queue = deque()
queue.append(task1)  # O(1)
queue.append(task2)  # O(1)
task = queue.popleft()  # O(1) - fast!

# Example: Priority queue

# ‚ùå BAD: List with sorting
tasks = []
tasks.append((priority, task))
tasks.sort()  # O(n log n) every time!
next_task = tasks.pop(0)

# ‚úÖ GOOD: heapq (O(log n) operations)
import heapq

tasks = []
heapq.heappush(tasks, (priority, task))  # O(log n)
next_task = heapq.heappop(tasks)  # O(log n)

# Example: Range queries

# ‚ùå BAD: List (O(n) for range)
numbers = list(range(1000000))
result = [x for x in numbers if 1000 <= x <= 2000]  # Scans all!

# ‚úÖ GOOD: bisect for sorted data (O(log n) search)
import bisect

sorted_numbers = list(range(1000000))
start = bisect.bisect_left(sorted_numbers, 1000)
end = bisect.bisect_right(sorted_numbers, 2000)
result = sorted_numbers[start:end]  # Fast range extraction
```

### Caching Strategies

```python
"""
CACHING: Store computed results for reuse

Types:
1. Function result caching (memoization)
2. Database query caching
3. HTTP response caching
4. Distributed caching (Redis)
"""

# 1. Function result caching
from functools import lru_cache

@lru_cache(maxsize=128)
def expensive_computation(n):
    """Cache last 128 results"""
    time.sleep(1)  # Simulate expensive operation
    return n * n

# First call: slow (1 second)
result = expensive_computation(10)

# Second call: instant (cached)
result = expensive_computation(10)

# Manual cache with TTL (time-to-live)
import time
from functools import wraps

def timed_cache(seconds=60):
    """Cache with expiration"""
    cache = {}
  
    def decorator(func):
        @wraps(func)
        def wrapper(*args):
            now = time.time()
          
            # Check if cached and not expired
            if args in cache:
                result, timestamp = cache[args]
                if now - timestamp < seconds:
                    return result
          
            # Compute and cache
            result = func(*args)
            cache[args] = (result, now)
            return result
      
        return wrapper
    return decorator

@timed_cache(seconds=60)
def get_stock_price(symbol):
    """Cache stock price for 60 seconds"""
    return fetch_from_api(symbol)

# 2. Database query caching (Redis)
import redis
import json

cache = redis.Redis(host='localhost', port=6379)

def get_user(user_id):
    """Get user with caching"""
    # Try cache first
    cached = cache.get(f"user:{user_id}")
    if cached:
        return json.loads(cached)
  
    # Cache miss: query database
    user = db.query(User).filter_by(id=user_id).first()
  
    # Store in cache (expire after 5 minutes)
    cache.setex(
        f"user:{user_id}",
        300,  # 5 minutes
        json.dumps(user.to_dict())
    )
  
    return user

# 3. HTTP response caching
from fastapi import FastAPI, Response

app = FastAPI()

@app.get("/products/{product_id}")
async def get_product(product_id: int, response: Response):
    """Cache HTTP response"""
    # Set cache headers
    response.headers["Cache-Control"] = "public, max-age=3600"  # 1 hour
    response.headers["ETag"] = generate_etag(product_id)
  
    product = get_product_from_db(product_id)
    return product

# 4. Cache invalidation
def update_user(user_id, data):
    """Update user and invalidate cache"""
    # Update database
    db.query(User).filter_by(id=user_id).update(data)
    db.commit()
  
    # Invalidate cache
    cache.delete(f"user:{user_id}")

# Cache-aside pattern (lazy loading)
def cache_aside_pattern(key):
    """
    1. Check cache
    2. If miss, query database
    3. Store in cache
    4. Return result
    """
    # Check cache
    value = cache.get(key)
    if value:
        return value
  
    # Query database
    value = db.query(key)
  
    # Store in cache
    cache.set(key, value, ex=3600)
  
    return value

# Write-through pattern (eager caching)
def write_through_pattern(key, value):
    """
    1. Write to database
    2. Write to cache
    3. Return
    """
    # Write to database
    db.insert(key, value)
  
    # Write to cache
    cache.set(key, value, ex=3600)
```

(Part 7 continues with sections 7.3, 7.4, and 7.5...)

### Database Optimization

```python
"""
DATABASE OPTIMIZATION: Query and schema optimization

Techniques:
1. Use indexes
2. Avoid N+1 queries
3. Use query optimization
4. Connection pooling
5. Query result caching
"""

# 1. Index usage (covered in Part 3, but key points)

# ‚ùå BAD: No index on frequently queried column
class User(Base):
    __tablename__ = 'users'
  
    id = Column(Integer, primary_key=True)
    email = Column(String)  # No index!

# Query: SELECT * FROM users WHERE email = 'test@example.com'
# Without index: Full table scan O(n)

# ‚úÖ GOOD: Add index
class User(Base):
    __tablename__ = 'users'
  
    id = Column(Integer, primary_key=True)
    email = Column(String, index=True)  # Indexed!

# With index: B-tree lookup O(log n)

# 2. Avoid N+1 queries

# ‚ùå BAD: N+1 problem
users = session.query(User).all()  # 1 query
for user in users:
    orders = user.orders  # N queries!
    print(f"{user.name}: {len(orders)} orders")

# Total: 1 + N queries (very slow!)

# ‚úÖ GOOD: Eager loading
from sqlalchemy.orm import joinedload

users = session.query(User).options(
    joinedload(User.orders)
).all()  # 1 query with JOIN

for user in users:
    orders = user.orders  # No additional query!
    print(f"{user.name}: {len(orders)} orders")

# Total: 1 query (fast!)

# 3. Query optimization

# ‚ùå BAD: Load all data
users = session.query(User).all()
emails = [user.email for user in users]

# Loads unnecessary data (name, phone, etc.)

# ‚úÖ GOOD: Select only needed columns
emails = session.query(User.email).all()

# Only loads email column

# 4. Batch operations

# ‚ùå BAD: One by one
for data in large_dataset:
    user = User(**data)
    session.add(user)
    session.commit()  # Commit each! Very slow

# ‚úÖ GOOD: Bulk insert
users = [User(**data) for data in large_dataset]
session.bulk_save_objects(users)
session.commit()  # Single commit

# 5. Connection pooling
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    'postgresql://user:pass@localhost/db',
    poolclass=QueuePool,
    pool_size=10,  # Keep 10 connections
    max_overflow=20,  # Allow 20 more if needed
    pool_pre_ping=True,  # Check connection before use
    pool_recycle=3600  # Recycle after 1 hour
)

# Reuses connections (much faster than creating new ones)
```

### Frequently Asked Questions

**Q1: What's the fastest way to process large files in Python?**

**A:**

```python
"""
LARGE FILE PROCESSING STRATEGIES

Techniques:
1. Read line by line (memory efficient)
2. Use generators (lazy evaluation)
3. Chunked reading
4. Memory-mapped files
5. Parallel processing
"""

# ‚ùå BAD: Load entire file into memory
def process_large_file_bad(filename):
    """Loads 10GB file into memory!"""
    with open(filename) as f:
        data = f.read()  # Memory error for large files!
  
    lines = data.split('\n')
    return [process_line(line) for line in lines]

# ‚úÖ GOOD: Read line by line
def process_large_file_good(filename):
    """Memory efficient"""
    results = []
    with open(filename) as f:
        for line in f:  # Reads one line at a time
            result = process_line(line)
            results.append(result)
    return results

# ‚úÖ BETTER: Use generator
def process_large_file_generator(filename):
    """Lazily yields results"""
    with open(filename) as f:
        for line in f:
            yield process_line(line)

# Use: for result in process_large_file_generator('huge.txt'):
#          handle_result(result)

# ‚úÖ BEST: Chunked reading with multiprocessing
from multiprocessing import Pool
import itertools

def process_chunk(lines):
    """Process chunk of lines"""
    return [process_line(line) for line in lines]

def process_large_file_parallel(filename, chunk_size=10000):
    """Process in parallel"""
    with Pool() as pool:
        with open(filename) as f:
            # Read in chunks
            while True:
                chunk = list(itertools.islice(f, chunk_size))
                if not chunk:
                    break
              
                # Process chunk in parallel
                results = pool.map(process_line, chunk)
                yield from results

# Memory-mapped files (fastest for random access)
import mmap

def process_with_mmap(filename):
    """Memory-mapped file access"""
    with open(filename, 'r+b') as f:
        # Map file to memory
        mmapped_file = mmap.mmap(f.fileno(), 0)
      
        # Fast random access
        data = mmapped_file[0:1000]  # Read bytes 0-1000
      
        mmapped_file.close()

# CSV specific: Use pandas for large CSVs
import pandas as pd

def process_large_csv(filename):
    """Process CSV in chunks"""
    chunk_size = 10000
  
    for chunk in pd.read_csv(filename, chunksize=chunk_size):
        # Process chunk
        processed = process_dataframe(chunk)
        yield processed

# Benchmark comparison:
"""
File size: 1 GB (10 million lines)

Method                    Time      Memory
----------------------------------------
Load all into memory      45s       10 GB    (crashes on 8GB RAM)
Line by line             30s       10 MB    ‚úÖ
Generator                30s       10 MB    ‚úÖ
Chunked + multiprocessing 8s       100 MB   ‚úÖ (4 cores)
Memory-mapped            15s       50 MB    ‚úÖ (random access)
"""
```

**Why This Matters:** Processing large files incorrectly causes out-of-memory errors.

---

## 7.3 Concurrency Optimization

**SDLC Phase:** Development, Performance Optimization

**Relevant For:**

- [ ] Requirements gathering
- [X] System design
- [X] Development
- [X] Testing (load testing)
- [ ] Deployment
- [X] Maintenance

```python
"""
CONCURRENCY OPTIMIZATION: Choose right concurrency model

Models:
1. Threading: I/O-bound tasks
2. Multiprocessing: CPU-bound tasks
3. Async/await: High-concurrency I/O

Decision tree:
- CPU-bound ‚Üí Multiprocessing
- I/O-bound (low concurrency) ‚Üí Threading
- I/O-bound (high concurrency) ‚Üí Async
"""

# 1. Threading for I/O-bound (network, disk)
from concurrent.futures import ThreadPoolExecutor
import requests

def download_url(url):
    """Download URL (I/O-bound)"""
    response = requests.get(url)
    return len(response.content)

# ‚ùå SLOW: Sequential
urls = ['http://example.com'] * 100
for url in urls:
    size = download_url(url)

# ‚úÖ FAST: Parallel with threads
with ThreadPoolExecutor(max_workers=10) as executor:
    sizes = list(executor.map(download_url, urls))

# 2. Multiprocessing for CPU-bound
from concurrent.futures import ProcessPoolExecutor

def cpu_intensive(n):
    """CPU-bound calculation"""
    total = 0
    for i in range(n):
        total += i ** 2
    return total

# ‚ùå SLOW: Sequential
results = [cpu_intensive(1000000) for _ in range(10)]

# ‚úÖ FAST: Parallel with processes
with ProcessPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(cpu_intensive, [1000000] * 10))

# 3. Async for high-concurrency I/O
import asyncio
import aiohttp

async def fetch_url(session, url):
    """Async HTTP request"""
    async with session.get(url) as response:
        return await response.text()

async def fetch_all(urls):
    """Fetch all URLs concurrently"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        return await asyncio.gather(*tasks)

# Run: asyncio.run(fetch_all(urls))

# Comparison (100 requests):
"""
Method              Time     Memory    Use Case
------------------------------------------------
Sequential          10s      10 MB     Baseline
Threading (10)      1.5s     50 MB     I/O-bound, low concurrency
Multiprocessing(4)  30s      200 MB    ‚ùå Wrong for I/O!
Async              0.5s     20 MB     I/O-bound, high concurrency ‚úÖ
"""

# Hybrid: CPU + I/O
import asyncio
from concurrent.futures import ProcessPoolExecutor

async def process_with_workers():
    """Combine async + multiprocessing"""
    loop = asyncio.get_event_loop()
  
    with ProcessPoolExecutor(max_workers=4) as pool:
        # CPU-bound in processes
        cpu_results = await loop.run_in_executor(
            pool,
            cpu_intensive,
            1000000
        )
      
        # I/O-bound async
        io_results = await fetch_all(urls)
  
    return cpu_results, io_results
```

## 7.4 Production Security

**SDLC Phase:** All Phases (Security is continuous)

**Relevant For:**

- [X] Requirements gathering (security requirements)
- [X] System design (secure architecture)
- [X] Development (secure coding)
- [X] Testing (security testing)
- [X] Deployment (secure deployment)
- [X] Maintenance (security updates)

```python
"""
PRODUCTION SECURITY BEST PRACTICES

OWASP Top 10 (2021):
1. Broken Access Control
2. Cryptographic Failures
3. Injection
4. Insecure Design
5. Security Misconfiguration
6. Vulnerable Components
7. Identification and Authentication Failures
8. Software and Data Integrity Failures
9. Security Logging and Monitoring Failures
10. Server-Side Request Forgery (SSRF)
"""

# 1. SQL Injection Prevention

# ‚ùå DANGEROUS: SQL injection vulnerability
def get_user_bad(email):
    query = f"SELECT * FROM users WHERE email = '{email}'"
    return db.execute(query)

# Attack: email = "'; DROP TABLE users; --"

# ‚úÖ SAFE: Parameterized queries
def get_user_safe(email):
    query = "SELECT * FROM users WHERE email = %s"
    return db.execute(query, (email,))

# SQLAlchemy (safe by default)
user = session.query(User).filter_by(email=email).first()

# 2. Password Security

# ‚ùå DANGEROUS: Plain text passwords
user.password = password  # Never!

# ‚ùå DANGEROUS: Weak hashing
import hashlib
user.password = hashlib.md5(password.encode()).hexdigest()  # Broken!

# ‚úÖ SAFE: Use bcrypt/argon2
from passlib.hash import bcrypt

def hash_password(password):
    """Securely hash password"""
    return bcrypt.hash(password)

def verify_password(password, hash):
    """Verify password"""
    return bcrypt.verify(password, hash)

# Register user
user.password_hash = hash_password(password)

# Login
if verify_password(entered_password, user.password_hash):
    # Success
    pass

# 3. XSS Prevention

# ‚ùå DANGEROUS: Unescaped user input
@app.get("/search")
def search(query: str):
    # Renders user input directly!
    return f"<h1>Results for {query}</h1>"

# Attack: query = "<script>alert('XSS')</script>"

# ‚úÖ SAFE: Escape output
from markupsafe import escape

@app.get("/search")
def search(query: str):
    safe_query = escape(query)
    return f"<h1>Results for {safe_query}</h1>"

# Or use templates (auto-escapes)
return templates.TemplateResponse("search.html", {"query": query})

# 4. CSRF Protection

# ‚úÖ Use CSRF tokens
from fastapi_csrf_protect import CsrfProtect

@app.post("/transfer")
async def transfer(
    amount: float,
    csrf_token: str = Depends(CsrfProtect.validate_csrf)
):
    # Process transfer
    pass

# 5. Authentication & Authorization

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer
import jwt

security = HTTPBearer()

def get_current_user(token: str = Depends(security)):
    """Verify JWT token"""
    try:
        payload = jwt.decode(token.credentials, SECRET_KEY, algorithms=["HS256"])
        user_id = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401)
        return user_id
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401)

def require_role(role: str):
    """Require specific role"""
    def checker(user_id: str = Depends(get_current_user)):
        user = get_user(user_id)
        if role not in user.roles:
            raise HTTPException(status_code=403)
        return user
    return checker

# Use in endpoints
@app.delete("/users/{user_id}")
async def delete_user(
    user_id: str,
    current_user = Depends(require_role("admin"))
):
    # Only admins can delete users
    pass

# 6. Rate Limiting

from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/login")
@limiter.limit("5/minute")  # Prevent brute force
async def login(email: str, password: str):
    # Login logic
    pass

# 7. Input Validation

from pydantic import BaseModel, EmailStr, constr, validator

class UserCreate(BaseModel):
    email: EmailStr  # Validates email format
    password: constr(min_length=8)  # Minimum 8 chars
    age: int
  
    @validator('age')
    def validate_age(cls, v):
        if v < 18 or v > 120:
            raise ValueError('Age must be between 18 and 120')
        return v
  
    @validator('password')
    def validate_password_strength(cls, v):
        if not any(c.isupper() for c in v):
            raise ValueError('Password must contain uppercase')
        if not any(c.isdigit() for c in v):
            raise ValueError('Password must contain digit')
        return v

# 8. Secrets Management

# ‚ùå DANGEROUS: Hardcoded secrets
API_KEY = "sk_live_abc123"  # Never!

# ‚ùå DANGEROUS: Secrets in code
DATABASE_PASSWORD = "password123"  # Never!

# ‚úÖ SAFE: Environment variables
import os

API_KEY = os.getenv("API_KEY")
DATABASE_PASSWORD = os.getenv("DATABASE_PASSWORD")

# ‚úÖ BETTER: Secret manager
import boto3

def get_secret(secret_name):
    """Get secret from AWS Secrets Manager"""
    client = boto3.client('secretsmanager')
    response = client.get_secret_value(SecretId=secret_name)
    return response['SecretString']

DATABASE_PASSWORD = get_secret("prod/database/password")

# 9. Security Headers

from fastapi import FastAPI
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from starlette.middleware.cors import CORSMiddleware

app = FastAPI()

# HTTPS only
app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["example.com", "*.example.com"]
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://example.com"],  # Specific origins
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

@app.middleware("http")
async def add_security_headers(request, call_next):
    """Add security headers"""
    response = await call_next(request)
  
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
  
    return response

# 10. Dependency Scanning

# Check for vulnerabilities
# $ pip install safety
# $ safety check

# In CI/CD
"""
- name: Security scan
  run: |
    pip install safety
    safety check --json
"""
```

## 7.5 Production Readiness

**SDLC Phase:** Deployment, Operations

**Relevant For:**

- [ ] Requirements gathering
- [X] System design
- [ ] Development
- [X] Testing
- [X] Deployment
- [X] Maintenance

```python
"""
PRODUCTION READINESS CHECKLIST

Categories:
1. Code Quality
2. Testing
3. Security
4. Performance
5. Observability
6. Operations
"""

# 1. Code Quality Checklist
code_quality = {
    "Linting": [
        "‚úÖ Black formatting",
        "‚úÖ Flake8 linting",
        "‚úÖ isort import sorting",
        "‚úÖ mypy type checking",
        "‚úÖ No pylint errors"
    ],
    "Code Review": [
        "‚úÖ PR reviewed by 2+ developers",
        "‚úÖ All comments addressed",
        "‚úÖ No hardcoded secrets",
        "‚úÖ No TODO comments",
        "‚úÖ Documentation updated"
    ],
    "Dependencies": [
        "‚úÖ All dependencies pinned",
        "‚úÖ No security vulnerabilities (safety check)",
        "‚úÖ Unused dependencies removed",
        "‚úÖ License compliance checked"
    ]
}

# 2. Testing Checklist
testing = {
    "Unit Tests": [
        "‚úÖ 80%+ code coverage",
        "‚úÖ All critical paths tested",
        "‚úÖ Edge cases tested",
        "‚úÖ Tests pass in CI/CD"
    ],
    "Integration Tests": [
        "‚úÖ Database integration tested",
        "‚úÖ API integration tested",
        "‚úÖ External service integration tested"
    ],
    "Performance Tests": [
        "‚úÖ Load tested (expected traffic)",
        "‚úÖ Stress tested (2x traffic)",
        "‚úÖ Response time < SLA",
        "‚úÖ No memory leaks",
        "‚úÖ Database queries optimized"
    ],
    "Security Tests": [
        "‚úÖ Penetration testing done",
        "‚úÖ OWASP Top 10 addressed",
        "‚úÖ Dependencies scanned",
        "‚úÖ Secrets not in code"
    ]
}

# 3. Observability Checklist
observability = {
    "Logging": [
        "‚úÖ Structured logging (JSON)",
        "‚úÖ Log levels correct",
        "‚úÖ Request ID in all logs",
        "‚úÖ Sensitive data not logged",
        "‚úÖ Log aggregation configured"
    ],
    "Metrics": [
        "‚úÖ Request rate tracked",
        "‚úÖ Error rate tracked",
        "‚úÖ Response time tracked",
        "‚úÖ Resource usage tracked (CPU, memory)",
        "‚úÖ Custom business metrics tracked"
    ],
    "Tracing": [
        "‚úÖ Distributed tracing enabled",
        "‚úÖ Critical paths traced",
        "‚úÖ External calls traced"
    ],
    "Alerting": [
        "‚úÖ High error rate alert",
        "‚úÖ High latency alert",
        "‚úÖ Service down alert",
        "‚úÖ Disk space alert",
        "‚úÖ On-call rotation configured"
    ]
}

# 4. Deployment Checklist
deployment = {
    "Infrastructure": [
        "‚úÖ Infrastructure as Code (Terraform/CloudFormation)",
        "‚úÖ All resources tagged",
        "‚úÖ Auto-scaling configured",
        "‚úÖ Multi-AZ deployment",
        "‚úÖ Backup configured",
        "‚úÖ Disaster recovery plan"
    ],
    "CI/CD": [
        "‚úÖ Automated tests in pipeline",
        "‚úÖ Automated security scanning",
        "‚úÖ Automated deployment",
        "‚úÖ Rollback plan tested",
        "‚úÖ Deployment can be paused"
    ],
    "Release": [
        "‚úÖ Changelog updated",
        "‚úÖ Release notes written",
        "‚úÖ Rollback plan documented",
        "‚úÖ Stakeholders notified",
        "‚úÖ Deployment window scheduled"
    ]
}

# 5. Operations Checklist
operations = {
    "Documentation": [
        "‚úÖ Architecture diagram",
        "‚úÖ Runbook for common issues",
        "‚úÖ Incident response plan",
        "‚úÖ On-call guide",
        "‚úÖ API documentation"
    ],
    "Monitoring": [
        "‚úÖ Health check endpoint",
        "‚úÖ Readiness probe",
        "‚úÖ Liveness probe",
        "‚úÖ Metrics dashboard",
        "‚úÖ Log dashboard"
    ],
    "Security": [
        "‚úÖ Secrets in vault",
        "‚úÖ SSL/TLS configured",
        "‚úÖ Firewall rules configured",
        "‚úÖ Access control configured",
        "‚úÖ Audit logging enabled"
    ]
}

# Production Readiness Score
def calculate_readiness_score(checklist):
    """Calculate production readiness percentage"""
    total = 0
    completed = 0
  
    for category, items in checklist.items():
        for item in items:
            total += 1
            if item.startswith("‚úÖ"):
                completed += 1
  
    return (completed / total) * 100

# All checklists
all_checklists = {
    **code_quality,
    **testing,
    **observability,
    **deployment,
    **operations
}

score = calculate_readiness_score(all_checklists)
print(f"Production Readiness: {score:.1f}%")

# Minimum requirements:
"""
Required for production:
- 80%+ overall score
- 100% security checklist
- 100% critical path testing
- 100% observability basics
"""
```

### Key Takeaways

**Performance & Production:**

- **Profiling**: cProfile (CPU), memory_profiler (memory), py-spy (production)
- **Optimization**: Choose right algorithm (O(n) vs O(n¬≤)), data structure, caching
- **Database**: Use indexes, avoid N+1, eager loading, connection pooling
- **Concurrency**: Threading (I/O), multiprocessing (CPU), async (high-concurrency I/O)
- **Security**: Parameterized queries, bcrypt passwords, escape output, JWT auth
- **Production**: Code quality, testing, observability, deployment automation

**Best Practices:**

- Profile before optimizing
- Optimize algorithms before micro-optimizations
- Cache expensive computations
- Use appropriate data structures
- Prevent SQL injection with parameterized queries
- Hash passwords with bcrypt
- Add security headers
- Monitor everything in production
- Have rollback plans

---

## Part 7 Complete!

You've completed **Part 7: Performance Optimization & Production Best Practices**!

‚úÖ **Part 1**: Python Fundamentals (~29,000 words)
‚úÖ **Part 2**: Python Internals (~20,000 words)
‚úÖ **Part 3**: SDLC & Architecture (~16,000 words)
‚úÖ **Part 4**: Testing & Quality (~9,200 words)
‚úÖ **Part 5**: Deployment & DevOps (~8,000 words)
‚úÖ **Part 6**: Version Control (~6,500 words)
‚úÖ **Part 7**: Performance & Production (~7,000 words)

**Total: ~95,700 words = ~383 pages!** üéä

You now have complete Python mastery from fundamentals through production optimization! üöÄ
